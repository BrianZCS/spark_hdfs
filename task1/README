Use the run.sh to run the task1. The program reads bunch of wiki data from HDFS and runs a pageRank program and output the final ranking to HDFS.

The program use default setting, where we didn't specify whether each rdd persist and how to partition the rdd. 

How to Run:
spark-3.3.4-bin-hadoop3/bin/spark-submit pageRank_wiki.py

